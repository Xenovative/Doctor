#!/usr/bin/env python3
"""
Comprehensive AI Analysis Unit Test Suite
Tests medical reference relevance and CHP guideline mapping accuracy
"""

import json
import requests
import time
from datetime import datetime
import sys
import os

class AIAnalysisTester:
    def __init__(self, base_url="http://localhost:7001"):
        self.base_url = base_url
        self.test_results = []
        self.chp_content = None

    def load_chp_content(self):
        """Load CHP content database for reference"""
        try:
            with open('assets/content.json', 'r', encoding='utf-8') as f:
                self.chp_content = json.load(f)
                print(f"âœ… Loaded {len(self.chp_content)} CHP entries")
        except Exception as e:
            print(f"âŒ Failed to load CHP content: {e}")
            return False
        return True

    def test_ai_analysis(self, symptoms, expected_chp_topics=None, test_name=""):
        """Test single AI analysis with symptom set"""
        print(f"\nğŸ§ª Testing: {test_name}")
        print(f"   Symptoms: {symptoms}")

        try:
            # Make request to the correct AI analysis endpoint
            # Based on the frontend code, it uses /find_doctor endpoint
            form_data = {
                "age": 30,
                "gender": "ç”·",
                "symptoms": symptoms,
                "language": "zh-TW",
                "location": "é¦™æ¸¯å³¶",
                "chronicConditions": "",
                "locationDetails": {
                    "region": "é¦™æ¸¯å³¶",
                    "district": "ä¸­è¥¿å€",
                    "area": "ä¸­ç’°"
                },
                "detailedHealthInfo": {
                    "height": "",
                    "weight": "",
                    "medications": "",
                    "allergies": "",
                    "surgeries": "",
                    "bloodThinner": False,
                    "recentVisit": False,
                    "cpapMachine": False,
                    "looseTeeth": False
                },
                "uiLanguage": "zh-TW"
            }

            response = requests.post(
                f"{self.base_url}/find_doctor",
                json=form_data,
                timeout=60  # Increased timeout for AI processing
            )

            if response.status_code != 200:
                return {
                    "test_name": test_name,
                    "symptoms": symptoms,
                    "status": "FAILED",
                    "error": f"HTTP {response.status_code}",
                    "response": None
                }

            result = response.json()

            # Check if we have analysis data
            if 'analysis' not in result:
                return {
                    "test_name": test_name,
                    "symptoms": symptoms,
                    "status": "FAILED",
                    "error": "No analysis data in response",
                    "response": result
                }

            analysis = result.get('analysis', '')

            # Extract symptoms from analysis for CHP mapping
            extracted_symptoms = self.extract_symptoms_from_analysis(analysis)

            # Test CHP relevance
            chp_relevance = self.test_chp_relevance(extracted_symptoms, expected_chp_topics)

            # Test medical evidence gathering
            medical_evidence = self.test_medical_evidence_gathering(symptoms)

            test_result = {
                "test_name": test_name,
                "symptoms": symptoms,
                "extracted_symptoms": extracted_symptoms,
                "status": "PASSED",
                "chp_relevance": chp_relevance,
                "pubmed_relevance": pubmed_relevance,
                "medical_evidence": medical_evidence,
                "analysis_preview": analysis[:200] + "..." if len(analysis) > 200 else analysis
            }

            return test_result

        except Exception as e:
            return {
                "test_name": test_name,
                "symptoms": symptoms,
                "status": "FAILED",
                "error": str(e),
                "response": None
            }

    def extract_symptoms_from_analysis(self, analysis_text):
        """Extract medical terms from AI analysis text"""
        symptoms = []

        # Look for symptom sections in both Chinese and English analysis
        import re

        # Common patterns for symptom extraction - both languages
        patterns = [
            # Chinese patterns
            r'ç—‡ç‹€åˆ†æï¼š(.*?)(?=ç›¸é—œå°ˆç§‘|ç·Šæ€¥ç¨‹åº¦|è³‡è¨Š|$)',
            r'ä¸»è¦ç—‡ç‹€åŒ…æ‹¬ï¼š(.*?)(?=ç›¸é—œå°ˆç§‘|ç·Šæ€¥ç¨‹åº¦|è³‡è¨Š|$)',
            r'æ‚£è€…å‡ºç¾(.*?)(?=ç›¸é—œå°ˆç§‘|ç·Šæ€¥ç¨‹åº¦|è³‡è¨Š|$)',
            # English patterns
            r'Symptoms include:(.*?)(?=Specialty|Emergency|Information|$)',
            r'Patient presents with(.*?)(?=Specialty|Emergency|Information|$)',
            r'Key symptoms:(.*?)(?=Specialty|Emergency|Information|$)'
        ]

        for pattern in patterns:
            match = re.search(pattern, analysis_text, re.DOTALL | re.IGNORECASE)
            if match:
                symptom_text = match.group(1)
                # Extract medical terms - both Chinese and English
                medical_terms = re.findall(r'[\u4e00-\u9fff]{2,10}|[A-Za-z][a-z]+(?: [a-z]+)*', symptom_text)
                symptoms.extend(medical_terms)
                break

        # If no structured patterns found, extract all medical-sounding terms
        if not symptoms:
            # Look for common medical terms in the text
            medical_terms = re.findall(r'\b(?:acute|chronic|severe|mild|viral|bacterial|infection|syndrome|disease|disorder|condition)\b', analysis_text, re.IGNORECASE)
            chinese_medical = re.findall(r'[\u4e00-\u9fff]{2,6}(?:ç—‡|ç‚|ç—…|æ¯’|ç—›|ç€‰|ç†±)', analysis_text)
            symptoms.extend(medical_terms + chinese_medical)

        return list(set(symptoms))  # Remove duplicates

    def test_chp_relevance(self, symptoms, expected_topics=None):
        """Test CHP content relevance for given symptoms"""
        if not self.chp_content:
            return {"score": 0, "matched_topics": [], "error": "CHP content not loaded"}

        relevant_entries = []
        matched_symptoms = []

        # Symptom to CHP topic mapping (same as in medical-evidence.js)
        symptom_mappings = {
            # Cardiovascular
            'å¿ƒè‡Ÿç—…': ['å¿ƒè‡Ÿç—…'],
            'é«˜è¡€å£“': ['å¿ƒè‡Ÿç—…', 'é«˜è¡€å£“'],
            'èƒ¸ç—›': ['å¿ƒè‡Ÿç—…'],
            'heart': ['å¿ƒè‡Ÿç—…'],
            'cardiac': ['å¿ƒè‡Ÿç—…'],

            # Metabolic/Diabetes
            'ç³–å°¿ç—…': ['ç³–å°¿ç—…'],
            'å£æ¸´': ['ç³–å°¿ç—…'],
            'å¤šå°¿': ['ç³–å°¿ç—…'],
            'å¤šé£²': ['ç³–å°¿ç—…'],
            'é«”é‡æ¸›è¼•': ['ç³–å°¿ç—…'],
            'diabetes': ['ç³–å°¿ç—…'],
            'diabetic': ['ç³–å°¿ç—…'],

            # Respiratory/Infectious
            'æµæ„Ÿ': ['ä¹™å‹æµæ„Ÿå—œè¡€æ¡¿èŒæ„ŸæŸ“', 'å­£ç¯€æ€§æµæ„Ÿ', 'å­£ç¯€æµè¡Œæ€§æ„Ÿå†’'],
            'æ„Ÿå†’': ['2019å† ç‹€ç—…æ¯’ç—…', 'å­£ç¯€æµè¡Œæ€§æ„Ÿå†’'],
            'å’³å—½': ['2019å† ç‹€ç—…æ¯’ç—…', 'è‚ºç‚çƒèŒæ„ŸæŸ“', 'å­£ç¯€æµè¡Œæ€§æ„Ÿå†’'],
            'ç™¼ç‡’': ['2019å† ç‹€ç—…æ¯’ç—…', 'æ°´ç—˜', 'æ‰‹è¶³å£ç—…', 'å­£ç¯€æµè¡Œæ€§æ„Ÿå†’'],
            'å–‰åš¨ç—›': ['2019å† ç‹€ç—…æ¯’ç—…', 'çŒ©ç´…ç†±', 'å­£ç¯€æµè¡Œæ€§æ„Ÿå†’'],
            'å‘¼å¸å›°é›£': ['2019å† ç‹€ç—…æ¯’ç—…', 'è‚ºç‚çƒèŒæ„ŸæŸ“'],
            'è‚ºç‚': ['è‚ºç‚çƒèŒæ„ŸæŸ“', 'è‚ºç‚æ”¯åŸé«”æ„ŸæŸ“'],
            'é¼»å¡': ['2019å† ç‹€ç—…æ¯’ç—…', 'å­£ç¯€æµè¡Œæ€§æ„Ÿå†’'],
            'influenza': ['å­£ç¯€æ€§æµæ„Ÿ', 'å­£ç¯€æµè¡Œæ€§æ„Ÿå†’'],
            'flu': ['å­£ç¯€æ€§æµæ„Ÿ', 'å­£ç¯€æµè¡Œæ€§æ„Ÿå†’'],

            # Gastrointestinal
            'è…¹ç—›': ['è«¾å¦‚ç—…æ¯’æ„ŸæŸ“', 'é£Ÿç‰©ä¸­æ¯’', 'è…¸èƒƒç‚'],
            'è…¹ç€‰': ['è«¾å¦‚ç—…æ¯’æ„ŸæŸ“', 'é£Ÿç‰©ä¸­æ¯’', 'è…¸èƒƒç‚'],
            'å˜”å': ['è«¾å¦‚ç—…æ¯’æ„ŸæŸ“', 'é£Ÿç‰©ä¸­æ¯’', 'è…¸èƒƒç‚'],
            'èƒƒç—›': ['è…¸èƒƒç‚', 'æ¶ˆåŒ–ä¸è‰¯'],
            'å™å¿ƒ': ['è…¸èƒƒç‚', 'é£Ÿç‰©ä¸­æ¯’'],
            'èƒƒè…¸': ['è…¸èƒƒç‚'],
            'food poisoning': ['é£Ÿç‰©ä¸­æ¯’'],
            'gastroenteritis': ['è…¸èƒƒç‚'],
            'diarrhea': ['è«¾å¦‚ç—…æ¯’æ„ŸæŸ“'],
            'vomiting': ['è«¾å¦‚ç—…æ¯’æ„ŸæŸ“'],

            # Skin conditions
            'çš®ç–¹': ['æ°´ç—˜', 'æ‰‹è¶³å£ç—…', 'éº»ç–¹'],
            'æ°´æ³¡': ['æ°´ç—˜'],
            'å£è…”æ½°ç˜': ['æ‰‹è¶³å£ç—…'],
            'æ‰‹è¶³çš®ç–¹': ['æ‰‹è¶³å£ç—…'],
            'rash': ['æ°´ç—˜', 'æ‰‹è¶³å£ç—…'],
            'blister': ['æ°´ç—˜'],

            # Mental Health
            'æŠ‘é¬±': ['å¿ƒç†å¥åº·', 'æŠ‘é¬±ç—‡', 'ç²¾ç¥å¥åº·'],
            'ç„¦æ…®': ['å¿ƒç†å¥åº·', 'ç„¦æ…®ç—‡', 'ç²¾ç¥å¥åº·'],
            'å£“åŠ›å¤§': ['å¿ƒç†å¥åº·', 'å£“åŠ›ç®¡ç†', 'ç²¾ç¥å¥åº·'],
            'ç²¾ç¥': ['ç²¾ç¥å¥åº·'],
            'æƒ…ç·’': ['å¿ƒç†å¥åº·'],
            'depression': ['å¿ƒç†å¥åº·', 'æŠ‘é¬±ç—‡'],
            'anxiety': ['å¿ƒç†å¥åº·', 'ç„¦æ…®ç—‡'],
            'stress': ['å¿ƒç†å¥åº·', 'å£“åŠ›ç®¡ç†'],

            # Neurological
            'é ­ç—›': ['2019å† ç‹€ç—…æ¯’ç—…', 'åé ­ç—›'],
            'é ­æšˆ': ['å¿ƒè‡Ÿç—…', 'ç³–å°¿ç—…', 'è²§è¡€'],
            'ä¸­é¢¨': ['ä¸­é¢¨'],
            'headache': ['2019å† ç‹€ç—…æ¯’ç—…', 'åé ­ç—›'],
            'dizziness': ['å¿ƒè‡Ÿç—…', 'ç³–å°¿ç—…'],

            # Other
            'ç–²å€¦': ['ç³–å°¿ç—…', 'å¿ƒè‡Ÿç—…', 'è²§è¡€', 'ç”²ç‹€è…ºåŠŸèƒ½æ¸›é€€'],
            'é«”é‡': ['ç³–å°¿ç—…', 'ç‡Ÿé¤Š'],
            'ç‡Ÿé¤Š': ['é£²é£Ÿèˆ‡ç‡Ÿé¤Š'],
            'è²§è¡€': ['è²§è¡€'],
            'fatigue': ['ç³–å°¿ç—…', 'å¿ƒè‡Ÿç—…'],
            'tired': ['ç³–å°¿ç—…', 'å¿ƒè‡Ÿç—…']
        }

        for symptom in symptoms:
            for key, topics in symptom_mappings.items():
                if key in symptom or symptom in key:
                    matched_symptoms.append(symptom)
                    for topic in topics:
                        # Find actual CHP entries
                        chp_entries = [entry for entry in self.chp_content
                                     if entry.get('title', '').find(topic) >= 0]
                        relevant_entries.extend(chp_entries)
                    break

        # Remove duplicates
        seen_urls = set()
        unique_entries = []
        for entry in relevant_entries:
            if entry['url'] not in seen_urls:
                seen_urls.add(entry['url'])
                unique_entries.append(entry)

        # Calculate relevance score
        total_matched = len(matched_symptoms)
        total_entries = len(unique_entries)

        if total_matched == 0:
            relevance_score = 0
        else:
            # Score based on coverage and quality
            coverage_score = min(total_entries / max(total_matched, 1), 1.0) * 50
            quality_score = min(total_entries * 10, 50)  # Max 50 points for quality
            relevance_score = min(coverage_score + quality_score, 100)

        return {
            "score": round(relevance_score, 1),
            "matched_symptoms": matched_symptoms,
            "matched_topics": [entry['title'].replace('è¡ç”Ÿé˜²è­·ä¸­å¿ƒ - ', '') for entry in unique_entries[:3]],
            "total_entries": total_entries,
            "expected_topics": expected_topics or []
        }

    def test_pubmed_relevance(self, analysis_text, original_symptoms):
        """Test PubMed reference relevance"""
        # Enhanced scoring based on evidence quality indicators

        # Evidence quality indicators
        pubmed_mentions = analysis_text.count('PubMed') + analysis_text.count('é†«å­¸æ–‡ç»') + analysis_text.count('ç ”ç©¶')
        evidence_indicators = [
            'è‡¨åºŠè©¦é©—', 'è‡¨åºŠç ”ç©¶', 'é†«å­¸è­‰æ“š', 'ç³»çµ±æ€§å›é¡§', 'metaåˆ†æ',
            'clinical trial', 'clinical study', 'medical evidence', 'systematic review', 'meta-analysis',
            'ç ”ç©¶çµæœ', 'è­‰æ“šé¡¯ç¤º', 'æ–‡ç»æ”¯æŒ', 'é†«å­¸æœŸåˆŠ', 'peer-reviewed'
        ]

        # Count evidence quality indicators
        evidence_count = sum(1 for indicator in evidence_indicators if indicator in analysis_text)

        # Medical terminology indicators (shows scientific rigor)
        medical_terms = [
            'ç—…å› ', 'ç—…ç†', 'è¨ºæ–·', 'æ²»ç™‚', 'é å¾Œ', 'é é˜²', 'é¢¨éšªå› ç´ ',
            'etiology', 'pathology', 'diagnosis', 'treatment', 'prognosis', 'prevention', 'risk factors',
            'è‡¨åºŠç‰¹å¾µ', 'æµè¡Œç—…å­¸', 'ç”Ÿç‰©æ¨™è¨˜', 'æ²»ç™‚æ–¹æ¡ˆ', 'ç™‚æ•ˆ',
            'clinical features', 'epidemiology', 'biomarkers', 'treatment protocol', 'efficacy'
        ]
        medical_term_count = sum(1 for term in medical_terms if term in analysis_text)

        # Statistical and research methodology terms
        research_terms = [
            'çµ±è¨ˆ', 'é¡¯è‘—', 'ç›¸é—œ', 'é æ¸¬', 'åˆ†æ', 'æ¨¡å‹', 'æ•¸æ“š',
            'statistics', 'significant', 'correlation', 'prediction', 'analysis', 'model', 'data',
            'p-value', 'confidence interval', 'odds ratio', 'relative risk'
        ]
        research_term_count = sum(1 for term in research_terms if term in analysis_text)

        # Calculate comprehensive score
        base_score = min(pubmed_mentions * 10, 30)  # Max 30 for mentions
        evidence_bonus = min(evidence_count * 8, 25)  # Max 25 for quality indicators
        medical_bonus = min(medical_term_count * 5, 20)  # Max 20 for medical terminology
        research_bonus = min(research_term_count * 3, 15)  # Max 15 for research methodology

        total_score = min(base_score + evidence_bonus + medical_bonus + research_bonus, 100)

        return {
            "score": round(total_score, 1),
            "has_medical_references": pubmed_mentions > 0,
            "pubmed_mentions": pubmed_mentions,
            "evidence_indicators": evidence_count,
            "medical_terms": medical_term_count,
            "research_terms": research_term_count,
            "components": {
                "base_mentions": min(pubmed_mentions * 10, 30),
                "evidence_quality": evidence_bonus,
                "medical_terminology": medical_bonus,
                "research_methodology": research_bonus
            }
        }

    def test_medical_evidence_gathering(self, symptoms):
        """Test actual medical evidence gathering from the system"""
        try:
            # Test the medical evidence API directly
            evidence_response = requests.post(
                f"{self.base_url}/api/medical-evidence",
                json={"symptoms": symptoms},
                timeout=30
            )

            if evidence_response.status_code == 200:
                evidence_data = evidence_response.json()

                return {
                    "success": evidence_data.get("success", False),
                    "evidence_count": len(evidence_data.get("evidence", [])),
                    "evidence_titles": [entry.get("title", "") for entry in evidence_data.get("evidence", [])],
                    "evidence_sources": [entry.get("source", "") for entry in evidence_data.get("evidence", [])],
                    "has_pubmed": any("pubmed" in entry.get("url", "").lower() for entry in evidence_data.get("evidence", [])),
                    "has_chp": any("chp.gov.hk" in entry.get("url", "") for entry in evidence_data.get("evidence", [])),
                    "error": None
                }
            else:
                return {
                    "success": False,
                    "evidence_count": 0,
                    "evidence_titles": [],
                    "evidence_sources": [],
                    "has_pubmed": False,
                    "has_chp": False,
                    "error": f"API returned {evidence_response.status_code}"
                }

        except Exception as e:
            return {
                "success": False,
                "evidence_count": 0,
                "evidence_titles": [],
                "evidence_sources": [],
                "has_pubmed": False,
                "has_chp": False,
                "error": str(e)
            }

    def run_comprehensive_tests(self):
        """Run comprehensive test suite"""
        print("ğŸš€ Starting Comprehensive AI Analysis Test Suite")
        print("=" * 60)

        if not self.load_chp_content():
            print("âŒ Cannot proceed without CHP content")
            return []

        # Test cases with various symptom combinations
        test_cases = [
            # Respiratory infections
            {
                "name": "Common Cold Symptoms",
                "symptoms": ["å–‰åš¨ç—›", "é¼»å¡", "è¼•å¾®å’³å—½"],
                "expected_chp": ["2019å† ç‹€ç—…æ¯’ç—…"]
            },
            {
                "name": "Flu-like Symptoms",
                "symptoms": ["ç™¼ç‡’", "å’³å—½", "é ­ç—›", "å–‰åš¨ç—›"],
                "expected_chp": ["2019å† ç‹€ç—…æ¯’ç—…"]
            },
            {
                "name": "Severe Respiratory",
                "symptoms": ["é«˜ç‡’", "åŠ‡çƒˆå’³å—½", "å‘¼å¸å›°é›£"],
                "expected_chp": ["2019å† ç‹€ç—…æ¯’ç—…", "è‚ºç‚çƒèŒæ„ŸæŸ“"]
            },

            # Gastrointestinal
            {
                "name": "Food Poisoning",
                "symptoms": ["è…¹ç—›", "è…¹ç€‰", "å˜”å"],
                "expected_chp": ["è«¾å¦‚ç—…æ¯’æ„ŸæŸ“"]
            },
            {
                "name": "Stomach Issues",
                "symptoms": ["èƒƒç—›", "è…¹ç€‰", "å™å¿ƒ"],
                "expected_chp": ["è«¾å¦‚ç—…æ¯’æ„ŸæŸ“"]
            },

            # Chronic diseases
            {
                "name": "Diabetes Symptoms",
                "symptoms": ["å£æ¸´", "å¤šå°¿", "ç–²å€¦", "é«”é‡æ¸›è¼•"],
                "expected_chp": ["ç³–å°¿ç—…"]
            },
            {
                "name": "Heart Disease",
                "symptoms": ["èƒ¸ç—›", "å‘¼å¸å›°é›£", "ç–²å€¦"],
                "expected_chp": ["å¿ƒè‡Ÿç—…"]
            },
            {
                "name": "Hypertension",
                "symptoms": ["é ­ç—›", "é ­æšˆ", "é«˜è¡€å£“"],
                "expected_chp": ["å¿ƒè‡Ÿç—…"]
            },

            # Skin conditions
            {
                "name": "Chickenpox",
                "symptoms": ["ç™¼ç‡’", "çš®ç–¹", "æ°´æ³¡"],
                "expected_chp": ["æ°´ç—˜"]
            },
            {
                "name": "Hand Foot Mouth",
                "symptoms": ["ç™¼ç‡’", "å£è…”æ½°ç˜", "æ‰‹è¶³çš®ç–¹"],
                "expected_chp": ["æ‰‹è¶³å£ç—…"]
            },

            # Mental health
            {
                "name": "Mental Health",
                "symptoms": ["æŠ‘é¬±", "ç„¦æ…®", "å£“åŠ›å¤§"],
                "expected_chp": ["å¿ƒç†å¥åº·"]
            },

            # Mixed symptoms
            {
                "name": "Complex Case",
                "symptoms": ["ç™¼ç‡’", "å’³å—½", "èƒ¸ç—›", "ç–²å€¦"],
                "expected_chp": ["2019å† ç‹€ç—…æ¯’ç—…", "å¿ƒè‡Ÿç—…"]
            }
        ]

        all_results = []

        for i, test_case in enumerate(test_cases, 1):
            print(f"\nğŸ“Š Test {i}/{len(test_cases)}")

            result = self.test_ai_analysis(
                symptoms=test_case["symptoms"],
                expected_chp_topics=test_case["expected_chp"],
                test_name=test_case["name"]
            )

            all_results.append(result)

            # Show immediate results
            status_emoji = "âœ…" if result["status"] == "PASSED" else "âŒ"
            chp_score = result.get("chp_relevance", {}).get("score", 0)
            pubmed_score = result.get("pubmed_relevance", {}).get("score", 0)

            print(f"   {status_emoji} CHP Relevance: {chp_score}/100")
            print(f"   {status_emoji} PubMed Relevance: {pubmed_score}/100")

            # Show medical evidence gathering results
            evidence = result.get("medical_evidence", {})
            if evidence.get("success", False):
                evidence_count = evidence.get("evidence_count", 0)
                has_pubmed = evidence.get("has_pubmed", False)
                has_chp = evidence.get("has_chp", False)
                titles = evidence.get("evidence_titles", [])

                print(f"   ğŸ“š Evidence Gathered: {evidence_count} articles")
                if has_pubmed:
                    print(f"   ğŸ”¬ PubMed: âœ… ({sum(1 for t in titles if 'pubmed' in t.lower())} articles)")
                if has_chp:
                    print(f"   ğŸ¥ CHP: âœ… ({sum(1 for t in titles if 'chp' in t.lower() or 'è¡ç”Ÿ' in t)} articles)")

                if titles and len(titles) <= 3:
                    print(f"   ğŸ“– Titles: {', '.join(titles[:2])}")
            else:
                print(f"   âŒ Evidence Error: {evidence.get('error', 'Unknown')}")

            if result["status"] == "FAILED":
                print(f"   âŒ Error: {result.get('error', 'Unknown')}")

        return all_results

    def generate_report(self, results):
        """Generate comprehensive test report"""
        print("\n" + "="*80)
        print("ğŸ“Š COMPREHENSIVE AI ANALYSIS TEST REPORT")
        print("="*80)

        # Summary statistics
        total_tests = len(results)
        passed_tests = sum(1 for r in results if r["status"] == "PASSED")
        failed_tests = total_tests - passed_tests

        print("\nğŸ¯ OVERALL RESULTS:")
        print(f"   Total Tests: {total_tests}")
        print(f"   âœ… Passed: {passed_tests}")
        print(f"   âŒ Failed: {failed_tests}")
        print(f"   Success Rate: {(passed_tests/total_tests*100):.1f}%")

        # CHP Relevance Analysis
        chp_scores = [r.get("chp_relevance", {}).get("score", 0) for r in results if r["status"] == "PASSED"]
        avg_chp = 0.0
        if chp_scores:
            avg_chp = sum(chp_scores) / len(chp_scores)
            print("\nğŸ¥ CHP GUIDELINES ANALYSIS:")
            print(f"   Average Score: {avg_chp:.1f}/100")
            print(f"   Highest: {max(chp_scores)}/100")
            print(f"   Lowest: {min(chp_scores)}/100")

        # PubMed Relevance Analysis
        pubmed_scores = [r.get("pubmed_relevance", {}).get("score", 0) for r in results if r["status"] == "PASSED"]
        avg_pubmed = 0.0
        if pubmed_scores:
            avg_pubmed = sum(pubmed_scores) / len(pubmed_scores)
            print("\nğŸ“š PUBMED REFERENCES ANALYSIS:")
            print(f"   Average Score: {avg_pubmed:.1f}/100")
            print(f"   Highest: {max(pubmed_scores)}/100")
            print(f"   Lowest: {min(pubmed_scores)}/100")

        # Medical Evidence Gathering Analysis
        evidence_results = [r.get("medical_evidence", {}) for r in results if r["status"] == "PASSED"]
        successful_evidence = [e for e in evidence_results if e.get("success", False)]
        avg_evidence_count = 0.0
        total_pubmed_articles = 0
        total_chp_articles = 0

        if successful_evidence:
            evidence_counts = [e.get("evidence_count", 0) for e in successful_evidence]
            avg_evidence_count = sum(evidence_counts) / len(evidence_counts)

            # Count total articles from all evidence sources
            for evidence in successful_evidence:
                titles = evidence.get("evidence_titles", [])
                total_pubmed_articles += sum(1 for t in titles if 'pubmed' in t.lower())
                total_chp_articles += sum(1 for t in titles if 'chp' in t.lower() or 'è¡ç”Ÿ' in t)

            print("\nğŸ”¬ MEDICAL EVIDENCE GATHERING ANALYSIS:")
            print(f"   Successful Evidence Gathering: {len(successful_evidence)}/{len(evidence_results)} tests")
            print(f"   Average Articles per Test: {avg_evidence_count:.1f}")
            print(f"   Total PubMed Articles: {total_pubmed_articles}")
            print(f"   Total CHP Articles: {total_chp_articles}")
            print(f"   Evidence Integration Rate: {(len(successful_evidence)/len(evidence_results)*100):.1f}%")

        # Detailed results
        print("\nğŸ“‹ DETAILED TEST RESULTS:")
        print("-" * 80)

        for i, result in enumerate(results, 1):
            status = result["status"]
            status_emoji = "âœ…" if status == "PASSED" else "âŒ"

            print(f"\n{i}. {status_emoji} {result['test_name']}")

            if status == "PASSED":
                chp = result.get("chp_relevance", {})
                pubmed = result.get("pubmed_relevance", {})
                evidence = result.get("medical_evidence", {})

                print(f"   Symptoms: {', '.join(result['symptoms'])}")
                print(f"   Extracted: {', '.join(result.get('extracted_symptoms', []))}")
                print(f"   CHP Score: {chp.get('score', 0)}/100")
                print(f"   PubMed Score: {pubmed.get('score', 0)}/100")

                if chp.get("matched_topics"):
                    print(f"   CHP Topics: {', '.join(chp['matched_topics'][:2])}")

                # Show medical evidence gathering results
                if evidence.get("success", False):
                    evidence_count = evidence.get("evidence_count", 0)
                    print(f"   ğŸ“š Evidence Count: {evidence_count} articles")

                    titles = evidence.get("evidence_titles", [])
                    if titles:
                        print(f"   ğŸ“– Evidence Titles: {', '.join(titles[:2])}")

                    if evidence.get("has_pubmed", False):
                        pubmed_count = sum(1 for t in titles if 'pubmed' in t.lower())
                        print(f"   ğŸ”¬ PubMed Articles: {pubmed_count}")

                    if evidence.get("has_chp", False):
                        chp_count = sum(1 for t in titles if 'chp' in t.lower() or 'è¡ç”Ÿ' in t)
                        print(f"   ğŸ¥ CHP Articles: {chp_count}")
                else:
                    print(f"   âŒ Evidence Error: {evidence.get('error', 'API not available')}")

            else:
                print(f"   Error: {result.get('error', 'Unknown')}")

        # Recommendations
        print("\nğŸ’¡ RECOMMENDATIONS:")
        print("-" * 80)

        if avg_chp < 70:
            print("âš ï¸  CHP mapping needs improvement - consider expanding symptom coverage")

        if avg_pubmed < 60:
            print("âš ï¸  PubMed integration may need enhancement")

        if failed_tests > 0:
            print(f"âš ï¸  {failed_tests} tests failed - check AI analysis endpoint")

        print("\nâœ… Testing completed!")
        print(f"Report generated at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        return {
            "summary": {
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": failed_tests,
                "avg_chp_score": round(sum(chp_scores) / len(chp_scores), 1) if chp_scores else 0,
                "avg_pubmed_score": round(sum(pubmed_scores) / len(pubmed_scores), 1) if pubmed_scores else 0
            },
            "detailed_results": results
        }


def main():
    """Main test runner"""
    tester = AIAnalysisTester()

    # Run comprehensive tests
    results = tester.run_comprehensive_tests()

    # Generate report
    report = tester.generate_report(results)

    # Save results to file
    with open('ai_analysis_test_results.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print("\nğŸ’¾ Results saved to ai_analysis_test_results.json")
    return report


if __name__ == "__main__":
    main()
